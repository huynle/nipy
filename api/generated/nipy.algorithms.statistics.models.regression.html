<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Neuroimaging in Python &#8212; NIPY Documentation</title>
    
    <link rel="stylesheet" href="../../_static/nipy.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '0.5.0.dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="algorithms.statistics.models.setupscons" href="nipy.algorithms.statistics.models.setupscons.html" />
    <link rel="prev" title="algorithms.statistics.models.nlsmodel" href="nipy.algorithms.statistics.models.nlsmodel.html" />
  <meta name="keywords" content="nipy, neuroimaging, python, neuroscience, time
				 series">

  </head>
  <body role="document">
<div style="background-color: white; text-align: left; padding: 10px 10px 15px 15px">
 <a href="../../index.html">
  <img src="../../_static/reggie2.png" alt="NIPY logo"  border="0" />
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="nipy.algorithms.statistics.models.setupscons.html" title="algorithms.statistics.models.setupscons"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="nipy.algorithms.statistics.models.nlsmodel.html" title="algorithms.statistics.models.nlsmodel"
             accesskey="P">previous</a> |</li>
  <li><a href="../../index.html">NIPY home</a> |&nbsp;</li>

          <li class="nav-item nav-item-1"><a href="../../documentation.html" >NIPY documentation</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../index.html" accesskey="U">API</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">

  
<h4> Site Navigation </h4>
  <ul>
    <li><a href="../../documentation.html">Documentation</a></li>
    <li><a href="../../devel/index.html">Development</a></li>
  </ul>

<h4> NIPY Community </h4>
  <ul class="simple">
    <li><a class="reference external"
	href="http://nipy.org/">Community Home</a></li>
    <li><a class="reference external"
	href="http://nipy.org/project-directory">NIPY Projects</a></li>
    <li><a class="reference external"
	href="https://mail.python.org/mailman/listinfo/neuroimaging">Mailing List</a></li>
    <li><a class="reference external"
	href="license.html">License</a></li>
  </ul>

<h4> Github repo </h4>
  <ul class="simple">
    <li><a class="reference external"
	href="http://github.com/nipy/nipy/">Nipy Github</a></li>
  </ul>

  <h3><a href="../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">algorithms.statistics.models.regression</a><ul>
<li><a class="reference internal" href="#module-algorithms-statistics-models-regression">Module: <code class="docutils literal"><span class="pre">algorithms.statistics.models.regression</span></code></a></li>
<li><a class="reference internal" href="#classes">Classes</a><ul>
<li><a class="reference internal" href="#arestimator"><code class="docutils literal"><span class="pre">AREstimator</span></code></a></li>
<li><a class="reference internal" href="#armodel"><code class="docutils literal"><span class="pre">ARModel</span></code></a></li>
<li><a class="reference internal" href="#glsmodel"><code class="docutils literal"><span class="pre">GLSModel</span></code></a></li>
<li><a class="reference internal" href="#olsmodel"><code class="docutils literal"><span class="pre">OLSModel</span></code></a></li>
<li><a class="reference internal" href="#regressionresults"><code class="docutils literal"><span class="pre">RegressionResults</span></code></a></li>
<li><a class="reference internal" href="#wlsmodel"><code class="docutils literal"><span class="pre">WLSModel</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#functions">Functions</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="nipy.algorithms.statistics.models.nlsmodel.html"
                        title="previous chapter">algorithms.statistics.models.nlsmodel</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="nipy.algorithms.statistics.models.setupscons.html"
                        title="next chapter">algorithms.statistics.models.setupscons</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/api/generated/nipy.algorithms.statistics.models.regression.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>

<div id="searchbox-ml" style="display: none">
  <h3>Search mailing list archive</h3>
  <script type="text/javascript">
    function mlsearch(curobj)
    {
    curobj.q.value="site:http://mail.python.org/pipermail/neuroimaging/ "+curobj.userquery.value
    }
  </script>
  <form action="http://www.google.com/search" method="get" onSubmit="mlsearch(this)">
    <input name="userquery" size="13" type="text" /> <input type="submit" value="Go" />
    <input name="q" type="hidden" />
  </form>
</div>
  
<div id="searchbox-site" style="display: none">
  <h3>Search this site</h3>
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" size="13" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    </p>
</div>
<script type="text/javascript">$('#searchbox-ml').show(0);</script>
<script type="text/javascript">$('#searchbox-site').show(0);</script>


        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="algorithms-statistics-models-regression">
<h1>algorithms.statistics.models.regression<a class="headerlink" href="#algorithms-statistics-models-regression" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-algorithms-statistics-models-regression">
<h2>Module: <code class="xref py py-mod docutils literal"><span class="pre">algorithms.statistics.models.regression</span></code><a class="headerlink" href="#module-algorithms-statistics-models-regression" title="Permalink to this headline">¶</a></h2>
<p>Inheritance diagram for <code class="docutils literal"><span class="pre">nipy.algorithms.statistics.models.regression</span></code>:</p>
<img src="../../_images/inheritance-9c4b9fcf707ccd140de9305ab2490d2ed9159d4c.png" alt="Inheritance diagram of nipy.algorithms.statistics.models.regression" usemap="#inheritance7534ba070d" class="inheritance"/>
<map id="inheritance7534ba070d" name="inheritance7534ba070d">
<area shape="rect" id="node1" href="nipy.algorithms.statistics.models.model.html#nipy.algorithms.statistics.models.model.LikelihoodModel" target="_top" title="models.model.LikelihoodModel" alt="" coords="238,75,394,93"/>
<area shape="rect" id="node6" href="#nipy.algorithms.statistics.models.regression.OLSModel" target="_top" title="A simple ordinary least squares model." alt="" coords="440,75,585,93"/>
<area shape="rect" id="node2" href="nipy.algorithms.statistics.models.model.html#nipy.algorithms.statistics.models.model.Model" target="_top" title="A (predictive) statistical model." alt="" coords="43,75,152,93"/>
<area shape="rect" id="node3" href="nipy.algorithms.statistics.models.model.html#nipy.algorithms.statistics.models.model.LikelihoodModelResults" target="_top" title="Class to contain results from likelihood models" alt="" coords="4,39,192,58"/>
<area shape="rect" id="node8" href="#nipy.algorithms.statistics.models.regression.RegressionResults" target="_top" title="This class summarizes the fit of a linear regression model." alt="" coords="225,39,406,58"/>
<area shape="rect" id="node4" href="#nipy.algorithms.statistics.models.regression.AREstimator" target="_top" title="A class to estimate AR(p) coefficients from residuals" alt="" coords="20,4,175,22"/>
<area shape="rect" id="node5" href="#nipy.algorithms.statistics.models.regression.ARModel" target="_top" title="A regression model with an AR(p) covariance structure." alt="" coords="621,39,761,58"/>
<area shape="rect" id="node7" href="#nipy.algorithms.statistics.models.regression.GLSModel" target="_top" title="Generalized least squares model with a general covariance structure" alt="" coords="619,75,764,93"/>
<area shape="rect" id="node9" href="#nipy.algorithms.statistics.models.regression.WLSModel" target="_top" title="A regression model with diagonal but non&#45;identity covariance structure." alt="" coords="618,110,764,129"/>
</map>
<span class="target" id="module-nipy.algorithms.statistics.models.regression"></span><p>This module implements some standard regression models: OLS and WLS
models, as well as an AR(p) regression model.

Models are specified with a design matrix and are fit using their
&#8216;fit&#8217; method.

Subclasses that have more complicated covariance matrices
should write over the &#8216;whiten&#8217; method as the fit method
prewhitens the response by calling &#8216;whiten&#8217;.

General reference for regression models:

&#8216;Introduction to Linear Regression Analysis&#8217;, Douglas C. Montgomery,
    Elizabeth A. Peck, G. Geoffrey Vining. Wiley, 2006.</p>
</div>
<div class="section" id="classes">
<h2>Classes<a class="headerlink" href="#classes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="arestimator">
<h3><a class="reference internal" href="#nipy.algorithms.statistics.models.regression.AREstimator" title="nipy.algorithms.statistics.models.regression.AREstimator"><code class="xref py py-class docutils literal"><span class="pre">AREstimator</span></code></a><a class="headerlink" href="#arestimator" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nipy.algorithms.statistics.models.regression.AREstimator">
<em class="property">class </em><code class="descclassname">nipy.algorithms.statistics.models.regression.</code><code class="descname">AREstimator</code><span class="sig-paren">(</span><em>model</em>, <em>p=1</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.AREstimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>A class to estimate AR(p) coefficients from residuals</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.AREstimator.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>model</em>, <em>p=1</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.AREstimator.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Bias-correcting AR estimation class</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>model</strong> : <code class="docutils literal"><span class="pre">OSLModel</span></code> instance</p>
<blockquote>
<div><p>A models.regression.OLSmodel instance,
where <cite>model</cite> has attribute <code class="docutils literal"><span class="pre">design</span></code></p>
</div></blockquote>
<p><strong>p</strong> : int, optional</p>
<blockquote class="last">
<div><p>Order of AR(p) noise</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="armodel">
<h3><a class="reference internal" href="#nipy.algorithms.statistics.models.regression.ARModel" title="nipy.algorithms.statistics.models.regression.ARModel"><code class="xref py py-class docutils literal"><span class="pre">ARModel</span></code></a><a class="headerlink" href="#armodel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nipy.algorithms.statistics.models.regression.ARModel">
<em class="property">class </em><code class="descclassname">nipy.algorithms.statistics.models.regression.</code><code class="descname">ARModel</code><span class="sig-paren">(</span><em>design</em>, <em>rho</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.OLSModel" title="nipy.algorithms.statistics.models.regression.OLSModel"><code class="xref py py-class docutils literal"><span class="pre">nipy.algorithms.statistics.models.regression.OLSModel</span></code></a></p>
<p>A regression model with an AR(p) covariance structure.</p>
<p>In terms of a LikelihoodModel, the parameters
are beta, the usual regression parameters,
and sigma, a scalar nuisance parameter that
shows up as multiplier in front of the AR(p) covariance.</p>
<dl class="docutils">
<dt>The linear autoregressive process of order p&#8211;AR(p)&#8211;is defined as:</dt>
<dd>TODO</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nipy.algorithms.statistics.api</span> <span class="k">import</span> <span class="n">Term</span><span class="p">,</span> <span class="n">Formula</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">rec</span><span class="o">.</span><span class="n">fromarrays</span><span class="p">(([</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">9</span><span class="p">],</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)),</span>
<span class="gp">... </span>                         <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="s1">&#39;X&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">Formula</span><span class="p">([</span><span class="n">Term</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">),</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dmtx</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">design</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">return_float</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ARModel</span><span class="p">(</span><span class="n">dmtx</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>We go through the <code class="docutils literal"><span class="pre">model.iterative_fit</span></code> procedure long-hand:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">])</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AR coefficients:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">rho</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">rho</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">yule_walker</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;Y&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">results</span><span class="o">.</span><span class="n">predicted</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">df</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">df_resid</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">ARModel</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">design</span><span class="p">,</span> <span class="n">rho</span><span class="p">)</span> 
<span class="gp">...</span>
<span class="go">AR coefficients: [ 0.  0.]</span>
<span class="go">AR coefficients: [-0.61530877 -1.01542645]</span>
<span class="go">AR coefficients: [-0.72660832 -1.06201457]</span>
<span class="go">AR coefficients: [-0.7220361  -1.05365352]</span>
<span class="go">AR coefficients: [-0.72229201 -1.05408193]</span>
<span class="go">AR coefficients: [-0.722278   -1.05405838]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">theta</span> 
<span class="go">array([ 1.59564228, -0.58562172])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">t</span><span class="p">()</span> 
<span class="go">array([ 38.0890515 ,  -3.45429252])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">Tcontrast</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>  
<span class="go">&lt;T contrast: effect=-0.58562172384377043, sd=0.16953449108110835,</span>
<span class="go">t=-3.4542925165805847, df_den=5&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">Fcontrast</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>  
<span class="go">&lt;F contrast: F=4216.810299725842, df_den=5, df_num=2&gt;</span>
</pre></div>
</div>
<p>Reinitialize the model, and do the automated iterative fit</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">iterative_fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">],</span> <span class="n">niter</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">rho</span><span class="p">)</span>  
<span class="go">[-0.7220361  -1.05365352]</span>
</pre></div>
</div>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>design</em>, <em>rho</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize AR model instance</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>design</strong> : ndarray</p>
<blockquote>
<div><p>2D array with design matrix</p>
</div></blockquote>
<p><strong>rho</strong> : int or array-like</p>
<blockquote class="last">
<div><p>If int, gives order of model, and initializes rho to zeros.  If
ndarray, gives initial estimate of rho. Be careful as <code class="docutils literal"><span class="pre">ARModel(X,</span>
<span class="pre">1)</span> <span class="pre">!=</span> <span class="pre">ARModel(X,</span> <span class="pre">1.0)</span></code>.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>Y</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Fit model to data <cite>Y</cite></p>
<p>Full fit of the model including estimate of covariance matrix,
(whitened) residuals and scale.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>Y</strong> : array-like</p>
<blockquote>
<div><p>The dependent variable for the Least Squares problem.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>fit</strong> : RegressionResults</p>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.has_intercept">
<code class="descname">has_intercept</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.has_intercept" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Check if column of 1s is in column space of design</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.information">
<code class="descname">information</code><span class="sig-paren">(</span><em>beta</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.information" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Returns the information matrix at (beta, Y, nuisance).</p>
<p>See logL for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict</p>
<blockquote>
<div><p>A dict with key &#8216;sigma&#8217;, which is an estimate of sigma. If None,
defaults to its maximum likelihood estimate (with beta fixed) as
<code class="docutils literal"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code> where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>info</strong> : array</p>
<blockquote class="last">
<div><p>The information matrix, the negative of the inverse of the Hessian
of the of the log-likelihood function evaluated at (theta, Y,
nuisance).</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>design</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.initialize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.iterative_fit">
<code class="descname">iterative_fit</code><span class="sig-paren">(</span><em>Y</em>, <em>niter=3</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.iterative_fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Perform an iterative two-stage procedure to estimate AR(p)
parameters and regression coefficients simultaneously.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>data to which to fit model</p>
</div></blockquote>
<p><strong>niter</strong> : optional, int</p>
<blockquote>
<div><p>the number of iterations (default 3)</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">None</p>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.logL">
<code class="descname">logL</code><span class="sig-paren">(</span><em>beta</em>, <em>Y</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.logL" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Returns the value of the loglikelihood function at beta.</p>
<p>Given the whitened design matrix, the loglikelihood is evaluated
at the parameter vector, beta, for the dependent variable, Y
and the nuisance parameter, sigma.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key &#8216;sigma&#8217;, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>loglf</strong> : float</p>
<blockquote class="last">
<div><p>The value of the loglikelihood function.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The log-Likelihood Function is defined as</p>
<div class="math">
\[\ell(\beta,\sigma,Y)=
-\frac{n}{2}\log(2\pi\sigma^2) - \|Y-X\beta\|^2/(2\sigma^2)\]</div>
<p>The parameter <span class="math">\(\sigma\)</span> above is what is sometimes referred to as a
nuisance parameter. That is, the likelihood is considered as a function
of <span class="math">\(\beta\)</span>, but to evaluate it, a value of <span class="math">\(\sigma\)</span> is
needed.</p>
<p>If <span class="math">\(\sigma\)</span> is not provided, then its maximum likelihood estimate:</p>
<div class="math">
\[\hat{\sigma}(\beta) = \frac{\text{SSE}(\beta)}{n}\]</div>
<p>is plugged in. This likelihood is now a function of only <span class="math">\(\beta\)</span>
and is technically referred to as a profile-likelihood.</p>
<p class="rubric">References</p>
<table class="docutils citation" frame="void" id="r2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[R2]</a></td><td><ol class="first last upperalpha simple" start="23">
<li>Green.  &#8220;Econometric Analysis,&#8221; 5th ed., Pearson, 2003.</li>
</ol>
</td></tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>design=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>After a model has been fit, results are (assumed to be) stored
in self.results, which itself should have a predict method.</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.rank">
<code class="descname">rank</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.rank" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Compute rank of design matrix</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>beta</em>, <em>Y</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.score" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Gradient of the loglikelihood function at (beta, Y, nuisance).</p>
<p>The graient of the loglikelihood function at (beta, Y, nuisance) is the
score function.</p>
<p>See <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.ARModel.logL" title="nipy.algorithms.statistics.models.regression.ARModel.logL"><code class="xref py py-meth docutils literal"><span class="pre">logL()</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key &#8216;sigma&#8217;, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The gradient of the loglikelihood function.</p>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.whiten">
<code class="descname">whiten</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.whiten" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Whiten a series of columns according to AR(p) covariance structure</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>X</strong> : array-like of shape (n_features)</p>
<blockquote>
<div><p>array to whiten</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>wX</strong> : ndarray</p>
<blockquote class="last">
<div><p>X whitened with order self.order AR</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="glsmodel">
<h3><a class="reference internal" href="#nipy.algorithms.statistics.models.regression.GLSModel" title="nipy.algorithms.statistics.models.regression.GLSModel"><code class="xref py py-class docutils literal"><span class="pre">GLSModel</span></code></a><a class="headerlink" href="#glsmodel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel">
<em class="property">class </em><code class="descclassname">nipy.algorithms.statistics.models.regression.</code><code class="descname">GLSModel</code><span class="sig-paren">(</span><em>design</em>, <em>sigma</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.OLSModel" title="nipy.algorithms.statistics.models.regression.OLSModel"><code class="xref py py-class docutils literal"><span class="pre">nipy.algorithms.statistics.models.regression.OLSModel</span></code></a></p>
<p>Generalized least squares model with a general covariance structure</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>design</em>, <em>sigma</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>Y</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Fit model to data <cite>Y</cite></p>
<p>Full fit of the model including estimate of covariance matrix,
(whitened) residuals and scale.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>Y</strong> : array-like</p>
<blockquote>
<div><p>The dependent variable for the Least Squares problem.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>fit</strong> : RegressionResults</p>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.has_intercept">
<code class="descname">has_intercept</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.has_intercept" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Check if column of 1s is in column space of design</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.information">
<code class="descname">information</code><span class="sig-paren">(</span><em>beta</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.information" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Returns the information matrix at (beta, Y, nuisance).</p>
<p>See logL for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict</p>
<blockquote>
<div><p>A dict with key &#8216;sigma&#8217;, which is an estimate of sigma. If None,
defaults to its maximum likelihood estimate (with beta fixed) as
<code class="docutils literal"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code> where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>info</strong> : array</p>
<blockquote class="last">
<div><p>The information matrix, the negative of the inverse of the Hessian
of the of the log-likelihood function evaluated at (theta, Y,
nuisance).</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>design</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.initialize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.logL">
<code class="descname">logL</code><span class="sig-paren">(</span><em>beta</em>, <em>Y</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.logL" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Returns the value of the loglikelihood function at beta.</p>
<p>Given the whitened design matrix, the loglikelihood is evaluated
at the parameter vector, beta, for the dependent variable, Y
and the nuisance parameter, sigma.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key &#8216;sigma&#8217;, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>loglf</strong> : float</p>
<blockquote class="last">
<div><p>The value of the loglikelihood function.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The log-Likelihood Function is defined as</p>
<div class="math">
\[\ell(\beta,\sigma,Y)=
-\frac{n}{2}\log(2\pi\sigma^2) - \|Y-X\beta\|^2/(2\sigma^2)\]</div>
<p>The parameter <span class="math">\(\sigma\)</span> above is what is sometimes referred to as a
nuisance parameter. That is, the likelihood is considered as a function
of <span class="math">\(\beta\)</span>, but to evaluate it, a value of <span class="math">\(\sigma\)</span> is
needed.</p>
<p>If <span class="math">\(\sigma\)</span> is not provided, then its maximum likelihood estimate:</p>
<div class="math">
\[\hat{\sigma}(\beta) = \frac{\text{SSE}(\beta)}{n}\]</div>
<p>is plugged in. This likelihood is now a function of only <span class="math">\(\beta\)</span>
and is technically referred to as a profile-likelihood.</p>
<p class="rubric">References</p>
<table class="docutils citation" frame="void" id="r3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[R3]</a></td><td><ol class="first last upperalpha simple" start="23">
<li>Green.  &#8220;Econometric Analysis,&#8221; 5th ed., Pearson, 2003.</li>
</ol>
</td></tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>design=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>After a model has been fit, results are (assumed to be) stored
in self.results, which itself should have a predict method.</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.rank">
<code class="descname">rank</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.rank" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Compute rank of design matrix</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>beta</em>, <em>Y</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.score" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Gradient of the loglikelihood function at (beta, Y, nuisance).</p>
<p>The graient of the loglikelihood function at (beta, Y, nuisance) is the
score function.</p>
<p>See <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.GLSModel.logL" title="nipy.algorithms.statistics.models.regression.GLSModel.logL"><code class="xref py py-meth docutils literal"><span class="pre">logL()</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key &#8216;sigma&#8217;, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The gradient of the loglikelihood function.</p>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.whiten">
<code class="descname">whiten</code><span class="sig-paren">(</span><em>Y</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.whiten" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="olsmodel">
<h3><a class="reference internal" href="#nipy.algorithms.statistics.models.regression.OLSModel" title="nipy.algorithms.statistics.models.regression.OLSModel"><code class="xref py py-class docutils literal"><span class="pre">OLSModel</span></code></a><a class="headerlink" href="#olsmodel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel">
<em class="property">class </em><code class="descclassname">nipy.algorithms.statistics.models.regression.</code><code class="descname">OLSModel</code><span class="sig-paren">(</span><em>design</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nipy.algorithms.statistics.models.model.html#nipy.algorithms.statistics.models.model.LikelihoodModel" title="nipy.algorithms.statistics.models.model.LikelihoodModel"><code class="xref py py-class docutils literal"><span class="pre">nipy.algorithms.statistics.models.model.LikelihoodModel</span></code></a></p>
<p>A simple ordinary least squares model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>design</strong> : array-like</p>
<blockquote class="last">
<div><p>This is your design matrix.  Data are assumed to be column ordered with
observations in rows.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nipy.algorithms.statistics.api</span> <span class="k">import</span> <span class="n">Term</span><span class="p">,</span> <span class="n">Formula</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">rec</span><span class="o">.</span><span class="n">fromarrays</span><span class="p">(([</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)),</span>
<span class="gp">... </span>                         <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="s1">&#39;X&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">Formula</span><span class="p">([</span><span class="n">Term</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">),</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dmtx</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">design</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">return_float</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">OLSModel</span><span class="p">(</span><span class="n">dmtx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">theta</span>
<span class="go">array([ 0.25      ,  2.14285714])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="go">array([ 0.98019606,  1.87867287])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">Tcontrast</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>  
<span class="go">&lt;T contrast: effect=2.14285714286, sd=1.14062281591, t=1.87867287326,</span>
<span class="go">df_den=5&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">Fcontrast</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>  
<span class="go">&lt;F contrast: F=19.4607843137, df_den=5, df_num=2&gt;</span>
</pre></div>
</div>
<p class="rubric">Attributes</p>
<table border="1" class="docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>design</td>
<td>(ndarray) This is the design, or X, matrix.</td>
</tr>
<tr class="row-even"><td>wdesign</td>
<td>(ndarray) This is the whitened design matrix.  <cite>design</cite> == <cite>wdesign</cite> by default for the OLSModel, though models that inherit from the OLSModel will whiten the design.</td>
</tr>
<tr class="row-odd"><td>calc_beta</td>
<td>(ndarray) This is the Moore-Penrose pseudoinverse of the whitened design matrix.</td>
</tr>
<tr class="row-even"><td>normalized_cov_beta</td>
<td>(ndarray) <code class="docutils literal"><span class="pre">np.dot(calc_beta,</span> <span class="pre">calc_beta.T)</span></code></td>
</tr>
<tr class="row-odd"><td>df_resid</td>
<td>(scalar) Degrees of freedom of the residuals.  Number of observations less the rank of the design.</td>
</tr>
<tr class="row-even"><td>df_model</td>
<td>(scalar) Degrees of freedome of the model.  The rank of the design.</td>
</tr>
</tbody>
</table>
<p class="rubric">Methods</p>
<table border="1" class="docutils">
<colgroup>
<col width="72%" />
<col width="28%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>model.__init___(design)</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>model.logL(b=self.beta, Y)</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>design</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.__init__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>design</strong> : array-like</p>
<blockquote class="last">
<div><p>This is your design matrix.
Data are assumed to be column ordered with
observations in rows.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>Y</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Fit model to data <cite>Y</cite></p>
<p>Full fit of the model including estimate of covariance matrix,
(whitened) residuals and scale.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>Y</strong> : array-like</p>
<blockquote>
<div><p>The dependent variable for the Least Squares problem.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>fit</strong> : RegressionResults</p>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.has_intercept">
<code class="descname">has_intercept</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.has_intercept" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Check if column of 1s is in column space of design</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.information">
<code class="descname">information</code><span class="sig-paren">(</span><em>beta</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.information" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Returns the information matrix at (beta, Y, nuisance).</p>
<p>See logL for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict</p>
<blockquote>
<div><p>A dict with key &#8216;sigma&#8217;, which is an estimate of sigma. If None,
defaults to its maximum likelihood estimate (with beta fixed) as
<code class="docutils literal"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code> where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>info</strong> : array</p>
<blockquote class="last">
<div><p>The information matrix, the negative of the inverse of the Hessian
of the of the log-likelihood function evaluated at (theta, Y,
nuisance).</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>design</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.initialize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.logL">
<code class="descname">logL</code><span class="sig-paren">(</span><em>beta</em>, <em>Y</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.logL" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Returns the value of the loglikelihood function at beta.</p>
<p>Given the whitened design matrix, the loglikelihood is evaluated
at the parameter vector, beta, for the dependent variable, Y
and the nuisance parameter, sigma.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key &#8216;sigma&#8217;, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>loglf</strong> : float</p>
<blockquote class="last">
<div><p>The value of the loglikelihood function.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The log-Likelihood Function is defined as</p>
<div class="math">
\[\ell(\beta,\sigma,Y)=
-\frac{n}{2}\log(2\pi\sigma^2) - \|Y-X\beta\|^2/(2\sigma^2)\]</div>
<p>The parameter <span class="math">\(\sigma\)</span> above is what is sometimes referred to as a
nuisance parameter. That is, the likelihood is considered as a function
of <span class="math">\(\beta\)</span>, but to evaluate it, a value of <span class="math">\(\sigma\)</span> is
needed.</p>
<p>If <span class="math">\(\sigma\)</span> is not provided, then its maximum likelihood estimate:</p>
<div class="math">
\[\hat{\sigma}(\beta) = \frac{\text{SSE}(\beta)}{n}\]</div>
<p>is plugged in. This likelihood is now a function of only <span class="math">\(\beta\)</span>
and is technically referred to as a profile-likelihood.</p>
<p class="rubric">References</p>
<table class="docutils citation" frame="void" id="r4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[R4]</a></td><td><ol class="first last upperalpha simple" start="23">
<li>Green.  &#8220;Econometric Analysis,&#8221; 5th ed., Pearson, 2003.</li>
</ol>
</td></tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>design=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>After a model has been fit, results are (assumed to be) stored
in self.results, which itself should have a predict method.</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.rank">
<code class="descname">rank</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.rank" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Compute rank of design matrix</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>beta</em>, <em>Y</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.score" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Gradient of the loglikelihood function at (beta, Y, nuisance).</p>
<p>The graient of the loglikelihood function at (beta, Y, nuisance) is the
score function.</p>
<p>See <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.OLSModel.logL" title="nipy.algorithms.statistics.models.regression.OLSModel.logL"><code class="xref py py-meth docutils literal"><span class="pre">logL()</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key &#8216;sigma&#8217;, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The gradient of the loglikelihood function.</p>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.whiten">
<code class="descname">whiten</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.whiten" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Whiten design matrix</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>X</strong> : array</p>
<blockquote>
<div><p>design matrix</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>wX</strong> : array</p>
<blockquote class="last">
<div><p>This matrix is the matrix whose pseudoinverse is ultimately
used in estimating the coefficients. For OLSModel, it is
does nothing. For WLSmodel, ARmodel, it pre-applies
a square root of the covariance matrix to X.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="regressionresults">
<h3><a class="reference internal" href="#nipy.algorithms.statistics.models.regression.RegressionResults" title="nipy.algorithms.statistics.models.regression.RegressionResults"><code class="xref py py-class docutils literal"><span class="pre">RegressionResults</span></code></a><a class="headerlink" href="#regressionresults" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults">
<em class="property">class </em><code class="descclassname">nipy.algorithms.statistics.models.regression.</code><code class="descname">RegressionResults</code><span class="sig-paren">(</span><em>theta</em>, <em>Y</em>, <em>model</em>, <em>wY</em>, <em>wresid</em>, <em>cov=None</em>, <em>dispersion=1.0</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nipy.algorithms.statistics.models.model.html#nipy.algorithms.statistics.models.model.LikelihoodModelResults" title="nipy.algorithms.statistics.models.model.LikelihoodModelResults"><code class="xref py py-class docutils literal"><span class="pre">nipy.algorithms.statistics.models.model.LikelihoodModelResults</span></code></a></p>
<p>This class summarizes the fit of a linear regression model.</p>
<p>It handles the output of contrasts, estimates of covariance, etc.</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>theta</em>, <em>Y</em>, <em>model</em>, <em>wY</em>, <em>wresid</em>, <em>cov=None</em>, <em>dispersion=1.0</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>See LikelihoodModelResults constructor.</p>
<p>The only difference is that the whitened Y and residual values
are stored for a regression model.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.AIC">
<code class="descname">AIC</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.AIC" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Akaike Information Criterion</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.BIC">
<code class="descname">BIC</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.BIC" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Schwarz&#8217;s Bayesian Information Criterion</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.F_overall">
<code class="descname">F_overall</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.F_overall" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Overall goodness of fit F test,
comparing model to a model with just an intercept.
If not an OLS model this is a pseudo-F.</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.Fcontrast">
<code class="descname">Fcontrast</code><span class="sig-paren">(</span><em>matrix</em>, <em>dispersion=None</em>, <em>invcov=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.Fcontrast" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Compute an Fcontrast for a contrast matrix <cite>matrix</cite>.</p>
<p>Here, <cite>matrix</cite> M is assumed to be non-singular. More precisely</p>
<div class="math">
\[M pX pX' M'\]</div>
<p>is assumed invertible. Here, <span class="math">\(pX\)</span> is the generalized inverse of
the design matrix of the model. There can be problems in non-OLS models
where the rank of the covariance of the noise is not full.</p>
<p>See the contrast module to see how to specify contrasts.  In particular,
the matrices from these contrasts will always be non-singular in the
sense above.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>matrix</strong> : 1D array-like</p>
<blockquote>
<div><p>contrast matrix</p>
</div></blockquote>
<p><strong>dispersion</strong> : None or float, optional</p>
<blockquote>
<div><p>If None, use <code class="docutils literal"><span class="pre">self.dispersion</span></code></p>
</div></blockquote>
<p><strong>invcov</strong> : None or array, optional</p>
<blockquote>
<div><p>Known inverse of variance covariance matrix.
If None, calculate this matrix.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>f_res</strong> : <code class="docutils literal"><span class="pre">FContrastResults</span></code> instance</p>
<blockquote class="last">
<div><p>with attributes F, df_den, df_num</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>For F contrasts, we now specify an effect and covariance</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.MSE">
<code class="descname">MSE</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.MSE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Mean square (error)</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.MSR">
<code class="descname">MSR</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.MSR" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Mean square (regression)</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.MST">
<code class="descname">MST</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.MST" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Mean square (total)</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.R2">
<code class="descname">R2</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.R2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Return the adjusted R^2 value for each row of the response Y.</p>
<p class="rubric">Notes</p>
<p>Changed to the textbook definition of R^2.</p>
<p>See: Davidson and MacKinnon p 74</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.R2_adj">
<code class="descname">R2_adj</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.R2_adj" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Return the R^2 value for each row of the response Y.</p>
<p class="rubric">Notes</p>
<p>Changed to the textbook definition of R^2.</p>
<p>See: Davidson and MacKinnon p 74</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.SSE">
<code class="descname">SSE</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.SSE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Error sum of squares. If not from an OLS model this is &#8220;pseudo&#8221;-SSE.</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.SSR">
<code class="descname">SSR</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.SSR" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Regression sum of squares</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.SST">
<code class="descname">SST</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.SST" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Total sum of squares. If not from an OLS model this is &#8220;pseudo&#8221;-SST.</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.Tcontrast">
<code class="descname">Tcontrast</code><span class="sig-paren">(</span><em>matrix</em>, <em>store=('t'</em>, <em>'effect'</em>, <em>'sd')</em>, <em>dispersion=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.Tcontrast" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Compute a Tcontrast for a row vector <cite>matrix</cite></p>
<p>To get the t-statistic for a single column, use the &#8216;t&#8217; method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>matrix</strong> : 1D array-like</p>
<blockquote>
<div><p>contrast matrix</p>
</div></blockquote>
<p><strong>store</strong> : sequence, optional</p>
<blockquote>
<div><p>components of t to store in results output object.  Defaults to all
components (&#8216;t&#8217;, &#8216;effect&#8217;, &#8216;sd&#8217;).</p>
</div></blockquote>
<p><strong>dispersion</strong> : None or float, optional</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>res</strong> : <code class="docutils literal"><span class="pre">TContrastResults</span></code> object</p>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.conf_int">
<code class="descname">conf_int</code><span class="sig-paren">(</span><em>alpha=0.05</em>, <em>cols=None</em>, <em>dispersion=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.conf_int" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>The confidence interval of the specified theta estimates.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>alpha</strong> : float, optional</p>
<blockquote>
<div><p>The <cite>alpha</cite> level for the confidence interval.
ie., <cite>alpha</cite> = .05 returns a 95% confidence interval.</p>
</div></blockquote>
<p><strong>cols</strong> : tuple, optional</p>
<blockquote>
<div><p><cite>cols</cite> specifies which confidence intervals to return</p>
</div></blockquote>
<p><strong>dispersion</strong> : None or scalar</p>
<blockquote>
<div><p>scale factor for the variance / covariance (see class docstring and
<code class="docutils literal"><span class="pre">vcov</span></code> method docstring)</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>cis</strong> : ndarray</p>
<blockquote class="last">
<div><p><cite>cis</cite> is shape <code class="docutils literal"><span class="pre">(len(cols),</span> <span class="pre">2)</span></code> where each row contains [lower,
upper] for the given entry in <cite>cols</cite></p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>Confidence intervals are two-tailed.
TODO:
tails : string, optional</p>
<blockquote>
<div><cite>tails</cite> can be &#8220;two&#8221;, &#8220;upper&#8221;, or &#8220;lower&#8221;</div></blockquote>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="k">import</span> <span class="n">standard_normal</span> <span class="k">as</span> <span class="n">stan</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nipy.algorithms.statistics.models.regression</span> <span class="k">import</span> <span class="n">OLSModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">stan</span><span class="p">((</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">stan</span><span class="p">((</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">stan</span><span class="p">((</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">))))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.25</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">stan</span><span class="p">((</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">OLSModel</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">confidence_intervals</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">conf_int</span><span class="p">(</span><span class="n">cols</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.logL">
<code class="descname">logL</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.logL" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>The maximized log-likelihood</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.norm_resid">
<code class="descname">norm_resid</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.norm_resid" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Residuals, normalized to have unit length.</p>
<p class="rubric">Notes</p>
<p>Is this supposed to return &#8220;stanardized residuals,&#8221;
residuals standardized
to have mean zero and approximately unit variance?</p>
<p>d_i = e_i / sqrt(MS_E)</p>
<p>Where MS_E = SSE / (n - k)</p>
<dl class="docutils">
<dt>See: Montgomery and Peck 3.2.1 p. 68</dt>
<dd>Davidson and MacKinnon 15.2 p 662</dd>
</dl>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.predicted">
<code class="descname">predicted</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.predicted" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Return linear predictor values from a design matrix.</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.resid">
<code class="descname">resid</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.resid" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Residuals from the fit.</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.t">
<code class="descname">t</code><span class="sig-paren">(</span><em>column=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.t" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Return the (Wald) t-statistic for a given parameter estimate.</p>
<p>Use Tcontrast for more complicated (Wald) t-statistics.</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.vcov">
<code class="descname">vcov</code><span class="sig-paren">(</span><em>matrix=None</em>, <em>column=None</em>, <em>dispersion=None</em>, <em>other=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.vcov" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Variance/covariance matrix of linear contrast</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>matrix: (dim, self.theta.shape[0]) array, optional</strong></p>
<blockquote>
<div><p>numerical contrast specification, where <code class="docutils literal"><span class="pre">dim</span></code> refers to the
&#8216;dimension&#8217; of the contrast i.e. 1 for t contrasts, 1 or more
for F contrasts.</p>
</div></blockquote>
<p><strong>column: int, optional</strong></p>
<blockquote>
<div><p>alternative way of specifying contrasts (column index)</p>
</div></blockquote>
<p><strong>dispersion: float or (n_voxels,) array, optional</strong></p>
<blockquote>
<div><p>value(s) for the dispersion parameters</p>
</div></blockquote>
<p><strong>other: (dim, self.theta.shape[0]) array, optional</strong></p>
<blockquote>
<div><p>alternative contrast specification (?)</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">cov: (dim, dim) or (n_voxels, dim, dim) array</p>
<blockquote>
<div><p>the estimated covariance matrix/matrices</p>
</div></blockquote>
<p>Returns the variance/covariance matrix of a linear contrast of the</p>
<p>estimates of theta, multiplied by <cite>dispersion</cite> which will often be an</p>
<p>estimate of <cite>dispersion</cite>, like, sigma^2.</p>
<p>The covariance of interest is either specified as a (set of) column(s)</p>
<p class="last">or a matrix.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="wlsmodel">
<h3><a class="reference internal" href="#nipy.algorithms.statistics.models.regression.WLSModel" title="nipy.algorithms.statistics.models.regression.WLSModel"><code class="xref py py-class docutils literal"><span class="pre">WLSModel</span></code></a><a class="headerlink" href="#wlsmodel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel">
<em class="property">class </em><code class="descclassname">nipy.algorithms.statistics.models.regression.</code><code class="descname">WLSModel</code><span class="sig-paren">(</span><em>design</em>, <em>weights=1</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.OLSModel" title="nipy.algorithms.statistics.models.regression.OLSModel"><code class="xref py py-class docutils literal"><span class="pre">nipy.algorithms.statistics.models.regression.OLSModel</span></code></a></p>
<p>A regression model with diagonal but non-identity covariance structure.</p>
<p>The weights are presumed to be (proportional to the) inverse
of the variance of the observations.</p>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nipy.algorithms.statistics.api</span> <span class="k">import</span> <span class="n">Term</span><span class="p">,</span> <span class="n">Formula</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">rec</span><span class="o">.</span><span class="n">fromarrays</span><span class="p">(([</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)),</span>
<span class="gp">... </span>                         <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="s1">&#39;X&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">Formula</span><span class="p">([</span><span class="n">Term</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">),</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dmtx</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">design</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">return_float</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">WLSModel</span><span class="p">(</span><span class="n">dmtx</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">theta</span>
<span class="go">array([ 0.0952381 ,  2.91666667])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="go">array([ 0.35684428,  2.0652652 ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">Tcontrast</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>  
<span class="go">&lt;T contrast: effect=2.91666666667, sd=1.41224801095, t=2.06526519708,</span>
<span class="go">df_den=5&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">Fcontrast</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>  
<span class="go">&lt;F contrast: F=26.9986072423, df_den=5, df_num=2&gt;</span>
</pre></div>
</div>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>design</em>, <em>weights=1</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>Y</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Fit model to data <cite>Y</cite></p>
<p>Full fit of the model including estimate of covariance matrix,
(whitened) residuals and scale.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>Y</strong> : array-like</p>
<blockquote>
<div><p>The dependent variable for the Least Squares problem.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>fit</strong> : RegressionResults</p>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.has_intercept">
<code class="descname">has_intercept</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.has_intercept" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Check if column of 1s is in column space of design</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.information">
<code class="descname">information</code><span class="sig-paren">(</span><em>beta</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.information" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Returns the information matrix at (beta, Y, nuisance).</p>
<p>See logL for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict</p>
<blockquote>
<div><p>A dict with key &#8216;sigma&#8217;, which is an estimate of sigma. If None,
defaults to its maximum likelihood estimate (with beta fixed) as
<code class="docutils literal"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code> where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>info</strong> : array</p>
<blockquote class="last">
<div><p>The information matrix, the negative of the inverse of the Hessian
of the of the log-likelihood function evaluated at (theta, Y,
nuisance).</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.initialize">
<code class="descname">initialize</code><span class="sig-paren">(</span><em>design</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.initialize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.logL">
<code class="descname">logL</code><span class="sig-paren">(</span><em>beta</em>, <em>Y</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.logL" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Returns the value of the loglikelihood function at beta.</p>
<p>Given the whitened design matrix, the loglikelihood is evaluated
at the parameter vector, beta, for the dependent variable, Y
and the nuisance parameter, sigma.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key &#8216;sigma&#8217;, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>loglf</strong> : float</p>
<blockquote class="last">
<div><p>The value of the loglikelihood function.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The log-Likelihood Function is defined as</p>
<div class="math">
\[\ell(\beta,\sigma,Y)=
-\frac{n}{2}\log(2\pi\sigma^2) - \|Y-X\beta\|^2/(2\sigma^2)\]</div>
<p>The parameter <span class="math">\(\sigma\)</span> above is what is sometimes referred to as a
nuisance parameter. That is, the likelihood is considered as a function
of <span class="math">\(\beta\)</span>, but to evaluate it, a value of <span class="math">\(\sigma\)</span> is
needed.</p>
<p>If <span class="math">\(\sigma\)</span> is not provided, then its maximum likelihood estimate:</p>
<div class="math">
\[\hat{\sigma}(\beta) = \frac{\text{SSE}(\beta)}{n}\]</div>
<p>is plugged in. This likelihood is now a function of only <span class="math">\(\beta\)</span>
and is technically referred to as a profile-likelihood.</p>
<p class="rubric">References</p>
<table class="docutils citation" frame="void" id="r5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[R5]</a></td><td><ol class="first last upperalpha simple" start="23">
<li>Green.  &#8220;Econometric Analysis,&#8221; 5th ed., Pearson, 2003.</li>
</ol>
</td></tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>design=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>After a model has been fit, results are (assumed to be) stored
in self.results, which itself should have a predict method.</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.rank">
<code class="descname">rank</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.rank" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Compute rank of design matrix</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>beta</em>, <em>Y</em>, <em>nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.score" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Gradient of the loglikelihood function at (beta, Y, nuisance).</p>
<p>The graient of the loglikelihood function at (beta, Y, nuisance) is the
score function.</p>
<p>See <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.WLSModel.logL" title="nipy.algorithms.statistics.models.regression.WLSModel.logL"><code class="xref py py-meth docutils literal"><span class="pre">logL()</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key &#8216;sigma&#8217;, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The gradient of the loglikelihood function.</p>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.whiten">
<code class="descname">whiten</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.whiten" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Whitener for WLS model, multiplies by sqrt(self.weights)</p>
</dd></dl>

</div>
</div>
<div class="section" id="functions">
<h2>Functions<a class="headerlink" href="#functions" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="nipy.algorithms.statistics.models.regression.ar_bias_correct">
<code class="descclassname">nipy.algorithms.statistics.models.regression.</code><code class="descname">ar_bias_correct</code><span class="sig-paren">(</span><em>results</em>, <em>order</em>, <em>invM=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ar_bias_correct" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply bias correction in calculating AR(p) coefficients from <cite>results</cite></p>
<p>There is a slight bias in the rho estimates on residuals due to the
correlations induced in the residuals by fitting a linear model.  See
<a class="reference internal" href="#id10" id="id5">[Worsley2002]</a>.</p>
<p>This routine implements the bias correction described in appendix A.1 of
<a class="reference internal" href="#id10" id="id6">[Worsley2002]</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>results</strong> : ndarray or results object</p>
<blockquote>
<div><p>If ndarray, assume these are residuals, from a simple model.  If a
results object, with attribute <code class="docutils literal"><span class="pre">resid</span></code>, then use these for the
residuals. See Notes for more detail</p>
</div></blockquote>
<p><strong>order</strong> : int</p>
<blockquote>
<div><p>Order <code class="docutils literal"><span class="pre">p</span></code> of AR(p) model</p>
</div></blockquote>
<p><strong>invM</strong> : None or array</p>
<blockquote>
<div><p>Known bias correcting matrix for covariance.  If None, calculate from
<code class="docutils literal"><span class="pre">results.model</span></code></p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>rho</strong> : array</p>
<blockquote class="last">
<div><p>Bias-corrected AR(p) coefficients</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>If <cite>results</cite> has attributes <code class="docutils literal"><span class="pre">resid</span></code> and <code class="docutils literal"><span class="pre">scale</span></code>, then assume <code class="docutils literal"><span class="pre">scale</span></code>
has come from a fit of a potentially customized model, and we use that for
the sum of squared residuals.  In this case we also need
<code class="docutils literal"><span class="pre">results.df_resid</span></code>.  Otherwise we assume this is a simple Gaussian model,
like OLS, and take the simple sum of squares of the residuals.</p>
<p class="rubric">References</p>
<table class="docutils citation" frame="void" id="worsley2002" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[Worsley2002]</td><td>K.J. Worsley, C.H. Liao, J. Aston, V. Petre, G.H. Duncan,
F. Morales, A.C. Evans (2002) A General Statistical Analysis for fMRI
Data.  Neuroimage 15:1:15</td></tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="nipy.algorithms.statistics.models.regression.ar_bias_corrector">
<code class="descclassname">nipy.algorithms.statistics.models.regression.</code><code class="descname">ar_bias_corrector</code><span class="sig-paren">(</span><em>design</em>, <em>calc_beta</em>, <em>order=1</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ar_bias_corrector" title="Permalink to this definition">¶</a></dt>
<dd><p>Return bias correcting matrix for <cite>design</cite> and AR order <cite>order</cite></p>
<p>There is a slight bias in the rho estimates on residuals due to the
correlations induced in the residuals by fitting a linear model.  See
<a class="reference internal" href="#id10" id="id8">[Worsley2002]</a>.</p>
<p>This routine implements the bias correction described in appendix A.1 of
<a class="reference internal" href="#id10" id="id9">[Worsley2002]</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>design</strong> : array</p>
<blockquote>
<div><p>Design matrix</p>
</div></blockquote>
<p><strong>calc_beta</strong> : array</p>
<blockquote>
<div><p>Moore-Penrose pseudoinverse of the (maybe) whitened design matrix.
This is the matrix that, when applied to the (maybe whitened) data,
produces the betas.</p>
</div></blockquote>
<p><strong>order</strong> : int, optional</p>
<blockquote>
<div><p>Order p of AR(p) process</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>invM</strong> : array</p>
<blockquote class="last">
<div><p>Matrix to bias correct estimated covariance matrix
in calculating the AR coefficients</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<table class="docutils citation" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[Worsley2002]</td><td>K.J. Worsley, C.H. Liao, J. Aston, V. Petre, G.H. Duncan,
F. Morales, A.C. Evans (2002) A General Statistical Analysis for fMRI
Data.  Neuroimage 15:1:15</td></tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="nipy.algorithms.statistics.models.regression.isestimable">
<code class="descclassname">nipy.algorithms.statistics.models.regression.</code><code class="descname">isestimable</code><span class="sig-paren">(</span><em>C</em>, <em>D</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.isestimable" title="Permalink to this definition">¶</a></dt>
<dd><p>True if (Q, P) contrast <cite>C</cite> is estimable for (N, P) design <cite>D</cite></p>
<p>From an Q x P contrast matrix <cite>C</cite> and an N x P design matrix <cite>D</cite>, checks if
the contrast <cite>C</cite> is estimable by looking at the rank of <code class="docutils literal"><span class="pre">vstack([C,D])</span></code>
and verifying it is the same as the rank of <cite>D</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>C</strong> : (Q, P) array-like</p>
<blockquote>
<div><p>contrast matrix. If <cite>C</cite> has is 1 dimensional assume shape (1, P)</p>
</div></blockquote>
<p><strong>D: (N, P) array-like</strong></p>
<blockquote>
<div><p>design matrix</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>tf</strong> : bool</p>
<blockquote class="last">
<div><p>True if the contrast <cite>C</cite> is estimable on design <cite>D</cite></p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">isestimable</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">D</span><span class="p">)</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">isestimable</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">D</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="nipy.algorithms.statistics.models.regression.yule_walker">
<code class="descclassname">nipy.algorithms.statistics.models.regression.</code><code class="descname">yule_walker</code><span class="sig-paren">(</span><em>X</em>, <em>order=1</em>, <em>method='unbiased'</em>, <em>df=None</em>, <em>inv=False</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.yule_walker" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate AR(p) parameters from a sequence X using Yule-Walker equation.</p>
<p>unbiased or maximum-likelihood estimator (mle)</p>
<p>See, for example:</p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/Autoregressive_moving_average_model">http://en.wikipedia.org/wiki/Autoregressive_moving_average_model</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>X</strong> :  ndarray of shape(n)</p>
<p><strong>order</strong> : int, optional</p>
<blockquote>
<div><p>Order of AR process.</p>
</div></blockquote>
<p><strong>method</strong> : str, optional</p>
<blockquote>
<div><p>Method can be &#8220;unbiased&#8221; or &#8220;mle&#8221; and this determines denominator in
estimate of autocorrelation function (ACF) at lag k. If &#8220;mle&#8221;, the
denominator is n=X.shape[0], if &#8220;unbiased&#8221; the denominator is n-k.</p>
</div></blockquote>
<p><strong>df</strong> : int, optional</p>
<blockquote>
<div><p>Specifies the degrees of freedom. If df is supplied, then it is assumed
the X has df degrees of freedom rather than n.</p>
</div></blockquote>
<p><strong>inv</strong> : bool, optional</p>
<blockquote>
<div><p>Whether to return the inverse of the R matrix (see code)</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>rho</strong> : (<cite>order</cite>,) ndarray</p>
<p><strong>sigma</strong> : int</p>
<blockquote>
<div><p>standard deviation of the residuals after fit</p>
</div></blockquote>
<p><strong>R_inv</strong> : ndarray</p>
<blockquote class="last">
<div><p>If <cite>inv</cite> is True, also return the inverse of the R matrix</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>See also
<a class="reference external" href="http://en.wikipedia.org/wiki/AR_model#Calculation_of_the_AR_parameters">http://en.wikipedia.org/wiki/AR_model#Calculation_of_the_AR_parameters</a></p>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="nipy.algorithms.statistics.models.setupscons.html" title="algorithms.statistics.models.setupscons"
             >next</a> |</li>
        <li class="right" >
          <a href="nipy.algorithms.statistics.models.nlsmodel.html" title="algorithms.statistics.models.nlsmodel"
             >previous</a> |</li>
  <li><a href="../../index.html">NIPY home</a> |&nbsp;</li>

          <li class="nav-item nav-item-1"><a href="../../documentation.html" >NIPY documentation</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../index.html" >API</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2005-2017, Neuroimaging in Python team.
      Last updated on Mar 17, 2017.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.5.1.
    </div>
  </body>
</html>